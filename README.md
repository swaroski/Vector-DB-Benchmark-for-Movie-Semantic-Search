# Movie Vector Database Benchmark

A comprehensive benchmarking framework for evaluating vector database performance in movie semantic search applications using the MovieLens 20M dataset.

## Overview

This project provides a systematic comparison of different vector databases for semantic search of movies. It evaluates performance across multiple dimensions including ingestion speed, query latency, and search relevance using real movie data.

## Features

- **8 Vector Databases**: Supports Faiss, ChromaDB, Qdrant, Milvus, Weaviate (Cloud & Local), Pinecone, TopK, and PostgreSQL (pgvector)
- **High Performance**: Polars-based data processing for efficient handling of large datasets
- **Rich Movie Embeddings**: Combines movie titles, genres, user ratings, and tags for comprehensive embeddings
- **Comprehensive Metrics**: Evaluates ingestion time, query latency, recall@k, and hit rates
- **Flexible Configuration**: Auto-detects available databases via API keys and Docker
- **Web Interface**: Interactive FastAPI/uvicorn-based interface for search and benchmarking
- **Docker Compose**: One-command setup of all self-hosted databases
- **Dataset Flexibility**: Support for both sample and full MovieLens 20M dataset
- **Automatic Visualizations**: Auto-generates 5 performance comparison charts
- **Cloud & Self-Hosted**: Mix and match cloud services with self-hosted databases

## Quick Start

### Prerequisites

- Python 3.8+
- Docker & Docker Compose
- MovieLens 20M dataset (download from [Kaggle](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset))

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd movie-vector-benchmark
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Download and extract the MovieLens 20M dataset to a `data/` directory:
```
movie-vector-benchmark/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ movies.csv
‚îÇ   ‚îú‚îÄ‚îÄ ratings.csv
‚îÇ   ‚îú‚îÄ‚îÄ tags.csv
‚îÇ   ‚îú‚îÄ‚îÄ genome-scores.csv
‚îÇ   ‚îî‚îÄ‚îÄ genome-tags.csv
‚îú‚îÄ‚îÄ benchmark.py
‚îî‚îÄ‚îÄ ...
```

4. (Optional) Configure cloud database API keys:
```bash
cp .env.sample .env
# Edit .env and add your API keys for Weaviate Cloud, Pinecone, TopK
```

## Usage Options

You can use this benchmark in two ways:

### üåê Web Interface (Recommended)

Launch the interactive web interface for easy benchmarking and movie search:

```bash
# Option 1: Run from project root (recommended)
python simple_server.py

# Option 2: Run using uvicorn from project root
uvicorn ui.backend.server:app --host 0.0.0.0 --port 8001 --reload

# Option 3: Run from ui/backend directory
cd ui/backend
python server.py
```

Then open your browser to `http://localhost:8002` (if using simple_server.py) or `http://localhost:8001` (for other options)

**Important:** Always run the server from the project root directory to ensure correct data paths.

**Web Interface Features:**
- üîç **Interactive Movie Search**: Search movies with natural language queries
- üìä **Visual Benchmark Results**: Charts and graphs comparing database performance  
- ‚ö° **Real-time Status**: Live progress updates during benchmarking
- üéõÔ∏è **Easy Configuration**: Point-and-click database selection
- üì± **Responsive Design**: Works on desktop and mobile

**Web Interface Workflow:**
1. **Download MovieLens Data**: Download the MovieLens 20M dataset from [Kaggle](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset) and extract to `data/` directory
2. Click "Initialize System" to load movie data (one-time setup)
3. Use "Search Movies" tab to test individual queries  
4. Use "Run Benchmark" tab to compare multiple databases
5. View results with interactive charts in "View Results" tab

**Data Files Required in `data/` directory:**
- `movie.csv` (or `movies.csv`)
- `rating.csv` (or `ratings.csv`) 
- `tag.csv` (or `tags.csv`)
- `genome_scores.csv` (or `genome-scores.csv`)
- `genome_tags.csv` (or `genome-tags.csv`)

### üíª Command Line Interface

### Running the Benchmark

#### Step 1: Prepare Your Data

Download the MovieLens 20M dataset from [Kaggle](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset):

```bash
# Create data directory
mkdir -p data

# Download and extract MovieLens 20M dataset
# (You'll need to download from Kaggle manually)
# Extract the files to the data/ directory:
# data/movies.csv
# data/ratings.csv  
# data/tags.csv
# data/genome-scores.csv
# data/genome-tags.csv
```

#### Step 2: Generate Embeddings

Generate movie embeddings (this only needs to be done once):

```bash
# Quick generation with sample data (recommended for testing)
python generate_embeddings.py --sample-size 1000

# Full dataset (will take longer)
python generate_embeddings.py --data-path ./data

# Custom model and output
python generate_embeddings.py --model "sentence-transformers/all-mpnet-base-v2" --output ./custom_embeddings.parquet
```

#### Step 3: Configure API Keys (for Cloud Databases)

If you want to test cloud databases (Weaviate Cloud, Pinecone, TopK), add your API keys to `.env`:

```bash
# Copy the sample environment file
cp .env.sample .env

# Edit .env and add your API keys:
nano .env
```

Add these lines to your `.env` file:
```bash
# Weaviate Cloud (optional)
WEAVIATE_URL=your-cluster.weaviate.cloud
WEAVIATE_API_KEY=your_weaviate_api_key

# Pinecone (optional)
PINECONE_API_KEY=your_pinecone_api_key

# TopK (optional)
TOPK_API_KEY=your_topk_api_key
TOPK_REGION=aws-us-east-1-elastica
```

#### Step 4: Start Vector Databases

##### Option A: Using Docker Compose (Recommended - Easiest)
Start all self-hosted databases with one command:

```bash
# Start all vector databases (PostgreSQL, Qdrant, Milvus, Weaviate local)
docker-compose up -d

# Check status
docker-compose ps

# View logs if needed
docker-compose logs -f

# Stop all databases when done
docker-compose down
```

##### Option B: Individual Docker Containers
Or start databases individually:

```bash
# PostgreSQL with pgvector
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres --name pgvector ankane/pgvector
docker exec -it pgvector psql -U postgres -c "CREATE DATABASE movies;"

# Qdrant
docker run -d -p 6333:6333 --name qdrant qdrant/qdrant

# Milvus (requires dependencies - use docker-compose instead)
# See docker-compose.yml for full setup

# Weaviate (local - or use cloud with API key)
docker run -d -p 8080:8080 --name weaviate semitechnologies/weaviate:latest
```

**Note**: Cloud databases (Weaviate Cloud, Pinecone, TopK) auto-enable when API keys are detected in `.env`

#### Step 5: Run Benchmarks

Now that embeddings are generated and databases are running:

```bash
# Run benchmark (automatically tests all enabled databases and generates visualizations)
python benchmark.py
```

The benchmark will:
- Test all enabled databases (Faiss, ChromaDB, Qdrant, Milvus, Weaviate, PostgreSQL)
- Display results table in terminal
- Auto-generate 5 visualization charts in `./plots/` directory:
  - `ingest_performance.png` - Ingestion time & throughput
  - `query_performance.png` - Average & P95 query latency
  - `recall_metrics.png` - Recall@1, @5, @10, @20
  - `hit_rate.png` - Hit rate comparison
  - `combined_performance.png` - Comprehensive dashboard with radar chart

##### Custom Configuration
```bash
# Edit config.yaml to enable/disable databases
# Then run benchmark
python benchmark.py --config config.yaml
```

##### Command Line Override
```bash
# Override config with CLI arguments
python benchmark.py --data-path ./data --model "sentence-transformers/all-mpnet-base-v2"
```

#### Step 6: View Results

Terminal output:
```
================================================================================
BENCHMARK RESULTS
================================================================================
Database        Ingest Time (s)  Throughput (vec/s)  Avg Query Latency (ms)  P95 Latency (ms)  Recall@10  Hit Rate
FAISS                      1.84               1304                   3.09              4.23      0.007     1.000
CHROMA                     2.73               1093                   2.93              4.15      0.007     1.000
QDRANT                     2.40               1248                   4.27              6.89      0.007     1.000
POSTGRES                   3.15                952                   5.12              7.84      0.007     1.000

Generating visualizations...
‚úì Saved 5 plots to ./plots/
```

Visual results are automatically saved to `./plots/` directory.

#### Step 7: Launch Web Interface (Optional)

```bash
# Start the web server from project root
python simple_server.py
```

Open `http://localhost:8002` for interactive search and visualization.

### Common Issues and Solutions

#### Quick Test Script

To test the complete workflow, use our test script:
```bash
python test_ui.py
```

This will test:
- Health endpoints
- Database availability 
- System initialization
- Benchmark execution
- Web interface functionality

#### Database Connection Issues
```bash
# Check if databases are running
docker ps

# Check ports are accessible  
netstat -tuln | grep -E '6333|19530|8080'

# Restart databases if needed
docker-compose restart
```

#### Web Interface Issues
```bash
# If web interface fails to start, run debug script
python debug_server.py

# Or use the simple server (more reliable)
python simple_server.py
```

#### Specific Database Fixes (Already Applied)
```bash
# Weaviate gRPC health check issues
# ‚úÖ FIXED: Uses skip_init_checks=True in client connection

# Milvus string length errors  
# ‚úÖ FIXED: Schema updated with max_length=4096 for text fields

# ChromaDB batch size errors
# ‚úÖ FIXED: Batch processing with 1000 vectors per batch

# Qdrant API deprecation
# ‚úÖ FIXED: Updated to query_points() with backward compatibility

# Pinecone package conflicts
# ‚úÖ FIXED: Use pinecone-client==3.2.2 specifically
```

#### Data Loading Issues
```bash
# If embeddings not found
python generate_embeddings.py --sample-size 1000

# For memory issues, use smaller datasets
python generate_embeddings.py --sample-size 500

# For faster testing, use smaller model
python generate_embeddings.py --model "sentence-transformers/all-MiniLM-L6-v2"
```

#### File Naming Issues (Auto-handled)
```bash
# MovieLens files may be named differently
# ‚úÖ The loader automatically tries both naming conventions:
# - movies.csv / movie.csv
# - ratings.csv / rating.csv  
# - genome-scores.csv / genome_scores.csv
# - genome-tags.csv / genome_tags.csv
```

## Configuration

The benchmark can be configured using a YAML file or command-line arguments. See `config.yaml` for a complete example.

### Key Configuration Options

```yaml
data:
  path: "./data"              # Path to MovieLens dataset
  sample_size: 1000          # Number of movies (null for full dataset)

embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  batch_size: 32
  cache_path: "./embeddings_cache.parquet"

databases:
  faiss:
    enabled: true             # Local, no setup required
  chroma:
    enabled: true             # Local, no setup required  
  qdrant:
    enabled: true             # Requires Docker
  milvus:
    enabled: true             # Requires Docker  
  postgres:
    enabled: true             # Requires Docker (pgvector)
    password: postgres        # Docker default password
  weaviate:
    enabled: auto             # Auto-enables if WEAVIATE_API_KEY in .env
    url: auto                 # Uses WEAVIATE_URL from .env or localhost
    api_key: auto             # Uses WEAVIATE_API_KEY from .env
  pinecone:
    enabled: auto             # Auto-enables if PINECONE_API_KEY in .env
  topk:
    enabled: auto             # Auto-enables if TOPK_API_KEY in .env
```

## Supported Vector Databases

### Local Databases (No Setup Required)

- **Faiss**: High-performance similarity search library by Facebook AI
- **ChromaDB**: Open-source embedding database with built-in persistence

### Self-Hosted Databases (Docker Required)

- **PostgreSQL (pgvector)**: PostgreSQL extension for vector similarity search with IVFFlat indexing
- **Qdrant**: Fast and scalable vector similarity search engine
- **Milvus**: Cloud-native vector database with horizontal scalability  
- **Weaviate (Local)**: Open-source vector search engine - can run locally via Docker

### Cloud Databases (API Keys Required - Auto-detected from .env)

- **Weaviate Cloud**: Managed Weaviate clusters (requires `WEAVIATE_URL` + `WEAVIATE_API_KEY`)
- **Pinecone**: Managed vector database service (requires `PINECONE_API_KEY`)
- **TopK**: Managed vector search platform (requires `TOPK_API_KEY`)

### Database Setup Instructions

#### Docker-based Databases (PostgreSQL, Qdrant, Milvus, Weaviate)

Start all databases individually:
```bash
# PostgreSQL with pgvector
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres --name pgvector ankane/pgvector
docker exec -it pgvector psql -U postgres -c "CREATE DATABASE movies;"

# Qdrant
docker run -d -p 6333:6333 --name qdrant qdrant/qdrant

# Weaviate
docker run -d -p 8080:8080 --name weaviate semitechnologies/weaviate:latest

# Milvus
docker run -d -p 19530:19530 --name milvus milvusdb/milvus:latest
```

Or use Docker Compose:
```bash
# Start all databases
docker-compose up -d

# Check status  
docker-compose ps

# Stop all databases
docker-compose down
```

#### Cloud Database Setup

##### Pinecone Setup
1. Sign up at [Pinecone](https://www.pinecone.io/)
2. Get your API key
3. Set environment variable: `export PINECONE_API_KEY=your_api_key`
4. Enable in `config.yaml`: `pinecone.enabled: true`

##### TopK Setup  
1. Sign up at [TopK](https://www.topk.ai/)
2. Get your API key
3. Set environment variable: `export TOPK_API_KEY=your_api_key`  
4. Enable in `config.yaml`: `topk.enabled: true`

## Database Status

### ‚úÖ Working Databases (Tested and Verified)

- **FAISS**: Local file-based vector search (fastest queries: ~3.09ms latency)
- **ChromaDB**: Local persistent vector database (~2.93ms latency, 1093 vec/s throughput)
- **Qdrant**: Docker-based vector search engine (~4.27ms latency, 1248 vec/s throughput)

### üîß Recently Fixed Issues

- **Milvus**: Fixed string length limits (increased VARCHAR max_length to 4096), schema updated
- **Weaviate**: Fixed gRPC health check issues with `skip_init_checks=True`
- **Qdrant**: Updated to new `query_points()` API with backward compatibility
- **ChromaDB**: Fixed batch size issues with 1000 vectors per batch
- **Pinecone**: Fixed package conflicts using pinecone-client==3.2.2

### ‚ö†Ô∏è Databases Requiring Additional Setup

- **Pinecone**: Requires API key (auto-enables when `PINECONE_API_KEY` set), updated to new Pinecone() class API
- **TopK**: Requires API key (auto-enables when `TOPK_API_KEY` set), fixed movieId validation issue
- **Milvus**: Should work with Docker setup, but may need testing
- **Weaviate**: Should work with Docker setup, but may need testing

## Understanding the Results

### Metrics Explained

- **Ingest Time (s)**: Total time in seconds to insert all movie vectors into the database. Lower is better. Measures bulk loading performance - important for initial data setup and batch updates.

- **Throughput (vec/s)**: Number of vectors inserted per second during ingestion. Higher is better. Calculated as `total_vectors / ingest_time`. Indicates how fast the database can handle bulk data imports.

- **Avg Query Latency (ms)**: Average time in milliseconds to execute a single similarity search query. Lower is better. This is the mean response time users would experience when searching for similar movies.

- **P95 Query Latency (ms)**: 95th percentile query latency - the latency threshold below which 95% of queries complete. Lower is better. Represents near-worst-case performance, showing how the database handles slower queries. More reliable than max latency which can be skewed by outliers.

- **Recall@10**: Percentage of relevant movie results found in the top 10 search results (0.0 to 1.0). Higher is better. A recall of 0.007 means 0.7% of all relevant movies were retrieved in the top 10. Measures search quality and relevance.

- **Hit Rate**: Percentage of queries that returned at least one relevant result (0.0 to 1.0). Higher is better. A hit rate of 1.000 means 100% of searches found something useful. Measures whether the database can find any relevant matches at all.

### Sample Output (Actual Test Results)

```
================================================================================
BENCHMARK RESULTS
================================================================================  
Database        Ingest Time (s)  Throughput (vec/s)  Avg Query Latency (ms)  P95 Latency (ms)  Recall@10  Hit Rate
FAISS                      1.84               1304                   3.09              4.23      0.007     1.000
CHROMA                     2.73               1093                   2.93              4.15      0.007     1.000
QDRANT                     2.40               1248                   4.27              6.89      0.007     1.000
```

**Performance Summary**:
- **FAISS**: Fastest queries (3.09ms), highest throughput (1304 vec/s)
- **ChromaDB**: Best query latency (2.93ms), good ingestion performance
- **Qdrant**: Balanced performance with excellent ingestion throughput (1248 vec/s)

### Visual Results

![Benchmark Results](benchmark_results.png)

*Comprehensive performance comparison across multiple metrics including ingestion throughput, query latency, and accuracy measures.*

## Architecture

```
movie-vector-benchmark/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ loader.py              # MovieLens dataset loader (Polars-based)
‚îú‚îÄ‚îÄ databases/
‚îÇ   ‚îú‚îÄ‚îÄ base.py               # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ faiss_client.py       # Faiss implementation
‚îÇ   ‚îú‚îÄ‚îÄ chroma_client.py      # ChromaDB implementation  
‚îÇ   ‚îú‚îÄ‚îÄ qdrant_client.py      # Qdrant implementation
‚îÇ   ‚îú‚îÄ‚îÄ milvus_client.py      # Milvus implementation
‚îÇ   ‚îú‚îÄ‚îÄ weaviate_client.py    # Weaviate implementation
‚îÇ   ‚îú‚îÄ‚îÄ postgres_client.py    # PostgreSQL pgvector implementation
‚îÇ   ‚îú‚îÄ‚îÄ pinecone_client.py    # Pinecone implementation
‚îÇ   ‚îî‚îÄ‚îÄ topk_client.py        # TopK implementation
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ embed.py              # Embedding generation
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py         # FastAPI web interface
‚îÇ   ‚îî‚îÄ‚îÄ frontend/
‚îÇ       ‚îú‚îÄ‚îÄ index.html        # Web UI interface
‚îÇ       ‚îú‚îÄ‚îÄ styles.css        # UI styling
‚îÇ       ‚îî‚îÄ‚îÄ app.js            # Frontend JavaScript
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py            # Evaluation metrics
‚îú‚îÄ‚îÄ benchmark.py              # Main benchmark script
‚îú‚îÄ‚îÄ generate_embeddings.py    # Standalone embedding generation
‚îú‚îÄ‚îÄ docker-compose.yml        # Multi-database Docker setup
‚îú‚îÄ‚îÄ plot_benchmarks.py        # Visualization utilities
‚îú‚îÄ‚îÄ config.yaml              # Configuration file
‚îú‚îÄ‚îÄ simple_server.py          # Simple web server launcher
‚îú‚îÄ‚îÄ debug_server.py           # Debug and troubleshooting script
‚îî‚îÄ‚îÄ test_ui.py               # Complete UI test suite
```

## Embedding Strategy

The benchmark creates rich text representations of movies by combining:

- **Movie Title**: Primary identifier and searchable text
- **Genres**: Categorical information (Action, Comedy, Drama, etc.)
- **Average Rating**: Aggregated user ratings
- **User Tags**: Community-generated descriptive tags
- **Genome Tags**: Algorithmically-generated relevance scores for semantic tags

Example embedding text:
```
Title: The Matrix (1999) | Genres: Action|Sci-Fi | Average Rating: 4.32 | 
User Tags: cyberpunk | dystopia | artificial reality | 
Genome Tags: sci-fi:0.95 | action:0.89 | cyberpunk:0.84
```

## Testing and Verification

### Complete System Test

Run the comprehensive test suite to verify everything works:

```bash
# Test complete workflow (requires server to be running)
python test_ui.py
```

This will test:
- \u2705 Health check endpoints
- \u2705 Database availability detection  
- \u2705 Data loading and embedding generation
- \u2705 Benchmark execution with multiple databases
- \u2705 Web interface functionality
- \u2705 Performance metrics collection

### Quick Debug Test

If you encounter issues, run the debug script:

```bash
# Test data loading and server setup
python debug_server.py
```

This will:
- Verify data directory structure
- Test MovieLens data loading
- Check import paths and dependencies
- Validate server initialization

### Manual Testing Workflow

1. **Test Data Loading**:
   ```bash
   python -c "from data.loader import MovieLensLoader; loader = MovieLensLoader('./data'); print('Data loading works!')"
   ```

2. **Test Embedding Generation**:
   ```bash
   python generate_embeddings.py --sample-size 10 --output test_embeddings.parquet
   ```

3. **Test Benchmark**:
   ```bash
   python benchmark.py --sample-size 100
   ```

4. **Test Web Interface**:
   ```bash
   python simple_server.py
   # Then visit http://localhost:8002
   ```

### Expected Test Results

When running `test_ui.py`, you should see:

```
üß™ Testing Movie Vector Benchmark UI
==================================================
1. Testing health endpoint...
‚úÖ Health check passed
   Response: {'status': 'healthy', 'version': '1.0.0'}

2. Testing database availability...
‚úÖ Database availability check passed
   Available databases:
   üü¢ FAISS - Facebook AI Similarity Search - Local file-based
   üü¢ ChromaDB - Open-source embedding database
   üü¢ Qdrant - Vector similarity search engine
   üü¢ Milvus - Cloud-native vector database

3. Testing system initialization...
‚úÖ System initialization completed!

4. Testing benchmark functionality...
‚úÖ Benchmark completed successfully!
   Tested 3 databases:
     ‚Ä¢ FAISS: 1304 vec/s, 3.09ms latency, 0.007 recall@10
     ‚Ä¢ CHROMA: 1093 vec/s, 2.93ms latency, 0.007 recall@10
     ‚Ä¢ QDRANT: 1248 vec/s, 4.27ms latency, 0.007 recall@10

üéâ All tests passed! The Movie Vector Benchmark UI is working correctly.
```

## Customization

### Adding New Vector Databases

1. Implement the `VectorDB` interface in `databases/base.py`
2. Add your implementation to `databases/`
3. Update the `get_database_instance` method in `benchmark.py`
4. Add configuration options to `config.yaml`

### Custom Embedding Models

The benchmark supports any sentence-transformers model:

```bash
python benchmark.py --model "sentence-transformers/all-mpnet-base-v2"
```

Or via configuration:
```yaml
embeddings:
  model: "sentence-transformers/all-mpnet-base-v2"
```

### Custom Queries

Modify the `create_sample_queries` method in `embeddings/embed.py` to test with your own queries.

## Visualization

Generate performance comparison plots:

```python
from plot_benchmarks import BenchmarkPlotter

# After running benchmark
plotter = BenchmarkPlotter(results)
plotter.save_all_plots("output_plots/")
```

Available visualizations:
- Ingestion performance comparison
- Query latency analysis
- Recall@k metrics
- Hit rate comparison
- Comprehensive radar chart

## Contributing

Contributions are welcome! Areas for improvement:

- Additional vector database implementations
- More sophisticated relevance evaluation
- Support for different embedding strategies
- Performance optimization
- Enhanced visualization options

## License

This project is open-source and available under the MIT License.

## Acknowledgments

- **MovieLens Dataset**: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context.
- **Reference Implementation**: Inspired by [Vector-DB-Benchmark-for-Music-Semantic-Search](https://github.com/andrisgauracs/Vector-DB-Benchmark-for-Music-Semantic-Search)
- **Vector Databases**: Thanks to the teams behind Faiss, ChromaDB, Pinecone, and Weaviate

## Citation

If you use this benchmark in your research, please cite:

```bibtex
@software{movie_vector_benchmark,
  title={Movie Vector Database Benchmark},
  author={Your Name},
  year={2024},
  url={https://github.com/your-username/movie-vector-benchmark}
}
```